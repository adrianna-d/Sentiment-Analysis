{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY 0.59\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Use a sample of 2000 entries\n",
    "df = df.sample(n=2000, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['emotion']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'estimator__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'estimator__penalty': ['l1', 'l2', 'elasticnet']\n",
    "}\n",
    "\n",
    "# Perform Grid Search with OneVsRestClassifier\n",
    "grid_search = GridSearchCV(estimator=OneVsRestClassifier(lr_model), param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "y_pred = best_lr_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Save the best model as an HDF5 file\n",
    "joblib.dump(best_lr_model, 'best_logreg_model_v3.h5')\n",
    "print(\"Model saved as best_logreg_model_v3.h5\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Use a sample of 2000 entries\n",
    "df = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['emotion']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100,200, 300, 600, 1000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=lr_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "y_pred = best_lr_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Save the best model as an HDF5 file\n",
    "import joblib\n",
    "joblib.dump(best_lr_model, 'best_logreg_model_v0.h5')\n",
    "print(\"Model saved as best_logreg_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY 0.67\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Use a sample of 6000 entries\n",
    "df = df.sample(n=6000, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['emotion']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# Initialize SVM model with best parameters\n",
    "best_svm_model = SVC(**grid_search.best_params_)\n",
    "\n",
    "# Train the model on the full training data\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Save SVM model using joblib\n",
    "joblib.dump(best_svm_model, 'svm_model.pkl')\n",
    "\n",
    "# Optionally, you can also evaluate the model on the validation set\n",
    "y_pred = best_svm_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# You can also print other evaluation metrics like classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "#ACCURACY 0.66\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "df = df.sample(n=6000, random_state=42)  # Sample size for demonstration, adjust as needed\n",
    "df.dropna(inplace=True)  # Handle missing values\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load('word2vec_model_v1.bin')  # Replace with your actual file name\n",
    "\n",
    "# Example function to calculate cosine similarity between emotions\n",
    "def cosine_similarity(emotion1, emotion2):\n",
    "    vec1 = np.mean([word2vec_model.wv[word] for word in emotion1.split() if word in word2vec_model.wv], axis=0)\n",
    "    vec2 = np.mean([word2vec_model.wv[word] for word in emotion2.split() if word in word2vec_model.wv], axis=0)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cosine_sim\n",
    "    else:\n",
    "        return 0.0  # Default similarity score when one or both words are not in the vocabulary\n",
    "\n",
    "# Example: Automatically merge classes based on cosine similarity threshold\n",
    "similarity_threshold = 0.7\n",
    "emotion_mapping = {}\n",
    "\n",
    "# Identify emotions appearing fewer than 3 times\n",
    "rare_emotions = df['emotion'].value_counts()[df['emotion'].value_counts() < 3].index.tolist()\n",
    "\n",
    "# Initialize emotion mapping dictionary\n",
    "emotion_mapping = {}\n",
    "\n",
    "# Loop through rare emotions\n",
    "for emotion in rare_emotions:\n",
    "    max_similarity = -1.0\n",
    "    most_similar_emotion = None\n",
    "    \n",
    "    # Find the most similar and more frequent emotion\n",
    "    for other_emotion in df['emotion'].unique():\n",
    "        if other_emotion not in rare_emotions:\n",
    "            similarity = cosine_similarity(emotion, other_emotion)\n",
    "            if isinstance(similarity, (int, float)) and similarity > similarity_threshold and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar_emotion = other_emotion\n",
    "    \n",
    "    # Map rare emotion to the most similar emotion found\n",
    "    if most_similar_emotion:\n",
    "        emotion_mapping[emotion] = most_similar_emotion\n",
    "\n",
    "# Merge rare classes into more similar and frequent classes\n",
    "df['emotion'] = df['emotion'].map(emotion_mapping).fillna(df['emotion'])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['emotion']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# Initialize SVM model with best parameters\n",
    "best_svm_model = SVC(**grid_search.best_params_)\n",
    "\n",
    "# Train the model on the full training data\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Save SVM model using joblib\n",
    "joblib.dump(best_svm_model, 'svm_model_v1.pkl')\n",
    "\n",
    "# Optionally, evaluate the model on the validation set\n",
    "y_pred = best_svm_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY 0.50\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Use a sample of 2000 entries\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion'])\n",
    "y = df['emotion_encoded']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Adjust for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Save LSTM model\n",
    "model.save('lstm_model_multiclass_sample6000_nwords_100k.h5')\n",
    "print(\"Model saved as lstm_model_multiclass.h5\")\n",
    "\n",
    "#ACCURACY 0.75 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Function to extend LabelEncoder dynamically\n",
    "def extend_label_encoder(label_encoder, new_labels):\n",
    "    existing_labels = set(label_encoder.classes_)\n",
    "    for label in new_labels:\n",
    "        if label not in existing_labels:\n",
    "            label_encoder.classes_ = np.append(label_encoder.classes_, label)\n",
    "            existing_labels.add(label)\n",
    "    return label_encoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "# Use a sample of 100,000 entries\n",
    "df = df.sample(n=100000, random_state=42)\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Adjust for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=60, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Print evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names, labels=unique_labels))\n",
    "\n",
    "# Save LSTM model to a specified file\n",
    "model_file = 'LSTM_models\\samp100k_wor200k.h5'\n",
    "model.save(model_file)\n",
    "print(f\"Model saved as {model_file}\")\n",
    "\n",
    "# Example of extending LabelEncoder dynamically\n",
    "# Suppose during inference, new labels are encountered\n",
    "new_labels = ['new_label1', 'new_label2']\n",
    "label_encoder = extend_label_encoder(label_encoder, new_labels)\n",
    "\n",
    "# Now label_encoder can handle new labels during prediction\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "class_names = label_encoder.inverse_transform(np.unique(y_val))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "\n",
    "#ACCURACY 0.79 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_path = 'word2vec_model_v1.bin'  # Update with your path\n",
    "word2vec = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "# Function to extend LabelEncoder dynamically\n",
    "def extend_label_encoder(label_encoder, new_labels):\n",
    "    existing_labels = set(label_encoder.classes_)\n",
    "    for label in new_labels:\n",
    "        if label not in existing_labels:\n",
    "            label_encoder.classes_ = np.append(label_encoder.classes_, label)\n",
    "            existing_labels.add(label)\n",
    "    return label_encoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "# Use a sample of 100,000 entries\n",
    "df = df.sample(n=100000, random_state=42)\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "def cosine_similarity(emotion1, emotion2):\n",
    "    vec1 = np.mean([word2vec.wv[word] for word in emotion1.split() if word in word2vec.wv], axis=0)\n",
    "    vec2 = np.mean([word2vec.wv[word] for word in emotion2.split() if word in word2vec.wv], axis=0)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cosine_sim\n",
    "    else:\n",
    "        return 0.0  # Default similarity score when one or both words are not in the vocabulary\n",
    "\n",
    "\n",
    "# Merge rare emotions based on cosine similarity\n",
    "similarity_threshold = 0.7\n",
    "rare_emotions = df['emotion'].value_counts()[df['emotion'].value_counts() < 3].index.tolist()\n",
    "emotion_mapping = {}\n",
    "\n",
    "for emotion in rare_emotions:\n",
    "    max_similarity = -1.0\n",
    "    most_similar_emotion = None\n",
    "    \n",
    "    for other_emotion in df['emotion'].unique():\n",
    "        if other_emotion not in rare_emotions:\n",
    "            similarity = cosine_similarity(emotion, other_emotion)\n",
    "            if isinstance(similarity, (int, float)) and similarity > similarity_threshold and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar_emotion = other_emotion\n",
    "    \n",
    "    if most_similar_emotion:\n",
    "        emotion_mapping[emotion] = most_similar_emotion\n",
    "\n",
    "df['emotion'] = df['emotion'].map(emotion_mapping).fillna(df['emotion'])\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = Counter(y)\n",
    "print(\"Class distribution before resampling:\", class_distribution)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(len(word_index) + 1, 200000)  # Limit to top 200,000 words\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "    # Note: if the word is not found in the Word2Vec model, embedding_matrix[i] will remain as zeros\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=100,\n",
    "                    trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=60, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Save LSTM model to a specified file\n",
    "model_file = 'LSTM_models/samp100k_wor200k_w2v_v2.h5'\n",
    "model.save(model_file)\n",
    "print(f\"Model saved as {model_file}\")\n",
    "\n",
    "# Example of extending LabelEncoder dynamically\n",
    "# Suppose during inference, new labels are encountered\n",
    "new_labels = ['new_label1', 'new_label2']\n",
    "label_encoder = extend_label_encoder(label_encoder, new_labels)\n",
    "\n",
    "# Get list of emotion classes\n",
    "emotion_classes = label_encoder.classes_\n",
    "# Define the file path to save the classes\n",
    "output_file = 'WebApp\\emotion_classes_s100k.txt'\n",
    "# Write emotion classes to a text file\n",
    "with open(output_file, 'w') as file:\n",
    "    for emotion_class in emotion_classes:\n",
    "        file.write(emotion_class + '\\n')\n",
    "\n",
    "import pickle\n",
    "# Save Tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Ensemble methods can be explored by combining multiple trained models, such as through averaging predictions or using a VotingClassifier approach in scikit-learn.\n",
    "# For simplicity, here's an example of averaging predictions from two different models:\n",
    "\n",
    "# Example of ensemble averaging predictions\n",
    "# Load two different saved models\n",
    "model_file_1 = 'LSTM_models/samp100k_wor200k_w2v_best.h5'\n",
    "model_file_2 = 'LSTM_models\\samp100k_wor200k_w2v.h5'\n",
    "\n",
    "model_1 = tf.keras.models.load_model(model_file_1)\n",
    "model_2 = tf.keras.models.load_model(model_file_2)\n",
    "\n",
    "# Get predictions from each model\n",
    "y_pred_probs_1 = model_1.predict(X_val)\n",
    "y_pred_probs_2 = model_2.predict(X_val)\n",
    "\n",
    "# Average predictions\n",
    "y_pred_avg = (y_pred_probs_1 + y_pred_probs_2) / 2\n",
    "y_pred_avg_classes = np.argmax(y_pred_avg, axis=1)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "accuracy_avg = accuracy_score(y_val, y_pred_avg_classes)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy_avg}\")\n",
    "print(\"Ensemble Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_avg_classes))\n",
    "print(\"Ensemble Model Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_avg_classes, target_names=label_encoder.classes_))\n",
    "#ACCURACY VAL 0.74\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_path = 'word2vec_model_v1.bin'  # Update with your path\n",
    "word2vec = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "# Function to calculate cosine similarity between emotions\n",
    "def cosine_similarity(emotion1, emotion2):\n",
    "    vec1 = np.mean([word2vec.wv[word] for word in emotion1.split() if word in word2vec.wv], axis=0)\n",
    "    vec2 = np.mean([word2vec.wv[word] for word in emotion2.split() if word in word2vec.wv], axis=0)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cosine_sim\n",
    "    else:\n",
    "        return 0.0  # Default similarity score when one or both words are not in the vocabulary\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "\n",
    "# Sample 100,000 entries\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Merge rare emotions based on cosine similarity\n",
    "similarity_threshold = 0.7\n",
    "rare_emotions = df['emotion'].value_counts()[df['emotion'].value_counts() < 3].index.tolist()\n",
    "emotion_mapping = {}\n",
    "\n",
    "for emotion in rare_emotions:\n",
    "    max_similarity = -1.0\n",
    "    most_similar_emotion = None\n",
    "    \n",
    "    for other_emotion in df['emotion'].unique():\n",
    "        if other_emotion not in rare_emotions:\n",
    "            similarity = cosine_similarity(emotion, other_emotion)\n",
    "            if isinstance(similarity, (int, float)) and similarity > similarity_threshold and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar_emotion = other_emotion\n",
    "    \n",
    "    if most_similar_emotion:\n",
    "        emotion_mapping[emotion] = most_similar_emotion\n",
    "\n",
    "df['emotion'] = df['emotion'].map(emotion_mapping).fillna(df['emotion'])\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = Counter(y)\n",
    "print(\"Class distribution before resampling:\", class_distribution)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(len(word_index) + 1, 200000)  # Limit to top 200,000 words\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "    # Note: if the word is not found in the Word2Vec model, embedding_matrix[i] will remain as zeros\n",
    "\n",
    "# Define the Keras LSTM model function\n",
    "def create_lstm_model(embedding_dim=100, num_classes=len(label_encoder.classes_), max_sequence_length=100, word_index=None, embedding_matrix=None):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_sequence_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='output_layer'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Manual hyperparameter tuning\n",
    "batch_sizes = [64]\n",
    "epochs_values = [60,70]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for epochs in epochs_values:\n",
    "        print(f\"Training with batch size: {batch_size}, epochs: {epochs}\")\n",
    "        \n",
    "        # Create a new instance of the model for each iteration\n",
    "        model = create_lstm_model(embedding_dim=embedding_dim, num_classes=len(label_encoder.classes_), max_sequence_length=100, word_index=word_index, embedding_matrix=embedding_matrix)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"Validation accuracy: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "final_model = create_lstm_model(embedding_dim=embedding_dim, num_classes=len(label_encoder.classes_), max_sequence_length=100, word_index=word_index, embedding_matrix=embedding_matrix)\n",
    "final_model.fit(X_train, y_train, epochs=60, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Save your final model\n",
    "final_model.save('LSTM_model/lstm_model_s10k.h5')\n",
    "print(\"Final model saved.\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_path = 'word2vec_model_v1.bin'  # Update with your path\n",
    "word2vec = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "# Function to extend LabelEncoder dynamically\n",
    "def extend_label_encoder(label_encoder, new_labels):\n",
    "    existing_labels = set(label_encoder.classes_)\n",
    "    for label in new_labels:\n",
    "        if label not in existing_labels:\n",
    "            label_encoder.classes_ = np.append(label_encoder.classes_, label)\n",
    "            existing_labels.add(label)\n",
    "    return label_encoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "# Use a sample of 100,000 entries\n",
    "df = df.sample(n=100000, random_state=42)\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def cosine_similarity(emotion1, emotion2):\n",
    "    vec1 = np.mean([word2vec.wv[word] for word in emotion1.split() if word in word2vec.wv], axis=0)\n",
    "    vec2 = np.mean([word2vec.wv[word] for word in emotion2.split() if word in word2vec.wv], axis=0)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return cosine_sim\n",
    "    else:\n",
    "        return 0.0  # Default similarity score when one or both words are not in the vocabulary\n",
    "\n",
    "# Merge rare emotions based on cosine similarity\n",
    "similarity_threshold = 0.7\n",
    "rare_emotions = df['emotion'].value_counts()[df['emotion'].value_counts() < 3].index.tolist()\n",
    "emotion_mapping = {}\n",
    "\n",
    "for emotion in rare_emotions:\n",
    "    max_similarity = -1.0\n",
    "    most_similar_emotion = None\n",
    "    \n",
    "    for other_emotion in df['emotion'].unique():\n",
    "        if other_emotion not in rare_emotions:\n",
    "            similarity = cosine_similarity(emotion, other_emotion)\n",
    "            if isinstance(similarity, (int, float)) and similarity > similarity_threshold and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar_emotion = other_emotion\n",
    "    \n",
    "    if most_similar_emotion:\n",
    "        emotion_mapping[emotion] = most_similar_emotion\n",
    "\n",
    "df['emotion'] = df['emotion'].map(emotion_mapping).fillna(df['emotion'])\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = Counter(y)\n",
    "print(\"Class distribution before resampling:\", class_distribution)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(len(word_index) + 1, 200000)  # Limit to top 200,000 words\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "    # Note: if the word is not found in the Word2Vec model, embedding_matrix[i] will remain as zeros\n",
    "\n",
    "# Define LSTM model function for GridSearchCV\n",
    "def create_lstm_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=100,\n",
    "                        input_shape=(1,),\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [20, 30]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "keras_lstm_model = KerasClassifier(build_fn=create_lstm_model, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=keras_lstm_model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=3,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model on validation set\n",
    "best_model = grid_search_results.best_estimator_\n",
    "y_pred_probs = best_model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Best Model Accuracy on Validation Set: {accuracy}\")\n",
    "\n",
    "# Save best model to a specified file\n",
    "#model_file = 'LSTM_models/best_model.h5'\n",
    "#best_model.model.save(model_file)\n",
    "#print(f\"Best model saved as {model_file}\")\n",
    "\n",
    "# Example of extending LabelEncoder dynamically\n",
    "# Suppose during inference, new labels are encountered\n",
    "new_labels = ['new_label1', 'new_label2']\n",
    "label_encoder = extend_label_encoder(label_encoder, new_labels)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_path = 'WebApp\\word2vec_model_v1.bin'  # Update with your path\n",
    "word2vec_model = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "# Load dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "# Use a sample of 100,000 entries\n",
    "df = df.sample(n=100000, random_state=42)\n",
    "# Load the new list of emotions\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "new_emotions_file = 'WebApp\\emotion_classes_s100k.txt'\n",
    "\n",
    "def cosine_similarity(emotion1, emotion2, word2vec_model):\n",
    "    vec1 = np.mean([word2vec_model.wv[word] for word in emotion1.split() if word in word2vec_model.wv], axis=0)\n",
    "    vec2 = np.mean([word2vec_model.wv[word] for word in emotion2.split() if word in word2vec_model.wv], axis=0)\n",
    "    \n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        return float(cosine_sim) if not isinstance(cosine_sim, np.ndarray) else float(cosine_sim[0])\n",
    "    else:\n",
    "        return -1.0  # Return a value that indicates no similarity when vectors are not valid\n",
    "\n",
    "\n",
    "similarity_threshold = 0.8\n",
    "rare_emotions = df['emotion'].value_counts()[df['emotion'].value_counts() < 3].index.tolist()\n",
    "emotion_mapping = {}\n",
    "\n",
    "for emotion in rare_emotions:\n",
    "    max_similarity = -1.0\n",
    "    most_similar_emotion = None\n",
    "    \n",
    "    for other_emotion in df['emotion'].unique():\n",
    "        if other_emotion not in rare_emotions and emotion != other_emotion:\n",
    "            similarity = cosine_similarity(emotion, other_emotion, word2vec_model)\n",
    "            if similarity != -1.0 and similarity > similarity_threshold and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                most_similar_emotion = other_emotion\n",
    "    \n",
    "    if most_similar_emotion is not None:\n",
    "        emotion_mapping[emotion] = most_similar_emotion\n",
    "\n",
    "df['emotion'] = df['emotion'].map(emotion_mapping).fillna(df['emotion'])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = np.bincount(y)\n",
    "print(\"Class distribution before resampling:\", class_distribution)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(len(word_index) + 1, 200000)  # Limit to top 200,000 words\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "    # Note: if the word is not found in the Word2Vec model, embedding_matrix[i] will remain as zeros\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=100,\n",
    "                    trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=60, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate LSTM model\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Save LSTM model to a specified file\n",
    "model_file = 'LSTM_models/lstm_retrained_list_v2.h5'\n",
    "model.save(model_file)\n",
    "print(f\"Model saved as {model_file}\")\n",
    "\n",
    "\n",
    "# Example of extending LabelEncoder dynamically\n",
    "new_labels = ['new_label1', 'new_label2']\n",
    "label_encoder.classes_ = np.append(label_encoder.classes_, new_labels)\n",
    "\n",
    "\n",
    "# Save Tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save Label Encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM_models/bilstm_emotion_classifier.h5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = np.bincount(y)\n",
    "print(\"Class distribution before resampling:\", class_distribution)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare embedding matrix using pre-trained Word2Vec embeddings\n",
    "# Assuming word2vec_model is loaded and processed as in your initial code\n",
    "word2vec_path = 'WebApp/word2vec_model_v1.bin'  # Update with your path\n",
    "word2vec_model = KeyedVectors.load(word2vec_path)\n",
    "\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(len(word_index) + 1, 200000)  # Limit to top 200,000 words\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    if word in word2vec_model.wv.key_to_index:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Define BiLSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=num_words,\n",
    "                         output_dim=embedding_dim,\n",
    "                         weights=[embedding_matrix],\n",
    "                         input_length=100,\n",
    "                         trainable=False))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_lstm.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train BiLSTM model with early stopping\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=50, batch_size=60, validation_data=(X_val, y_val), callbacks=[early_stopping_lstm])\n",
    "\n",
    "\n",
    "# Save BiLSTM model\n",
    "model_file_lstm = 'LSTM_models/bilstm_emotion_classifier.h5'\n",
    "model_lstm.save(model_file_lstm)\n",
    "print(f\"BiLSTM Model saved as {model_file_lstm}\")\n",
    "\n",
    "# Save Tokenizer and Label Encoder\n",
    "with open('tokenizer_bilstm.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('label_encoder_bilstm.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Evaluate BiLSTM model\n",
    "y_pred_probs_lstm = model_lstm.predict(X_val)\n",
    "y_pred_lstm = np.argmax(y_pred_probs_lstm, axis=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "accuracy_lstm = accuracy_score(y_val, y_pred_lstm)\n",
    "print(f\"Accuracy: {accuracy_lstm}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_lstm))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_lstm, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "df = df.sample(n=5000, random_state=42)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize texts\n",
    "max_len = 80  # Adjust according to your text length\n",
    "X = df['text'].astype(str).tolist()\n",
    "tokens = tokenizer(X, truncation=True, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "input_ids = tokens['input_ids'].numpy()\n",
    "attention_mask = tokens['attention_mask'].numpy()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, \n",
    "    attention_mask, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# BERT model architecture\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Taking pooled output as representation\n",
    "dropout = Dropout(0.1)(bert_output)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
    "model_bert = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "model_bert.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train BERT model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_bert = model_bert.fit(\n",
    "    {'input_ids': X_train_ids, 'attention_mask': X_train_mask}, \n",
    "    y_train, \n",
    "    epochs=25, \n",
    "    batch_size=32, \n",
    "    validation_data=({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate BERT model\n",
    "y_pred_probs_bert = model_bert.predict({'input_ids': X_val_ids, 'attention_mask': X_val_mask})\n",
    "y_pred_bert = np.argmax(y_pred_probs_bert, axis=1)\n",
    "\n",
    "# Save BERT model\n",
    "model_file_bert = 'BERT_models/bert_emotion_classifier_v1.keras'\n",
    "model_bert.save(model_file_bert)\n",
    "print(f\"BERT Model saved as {model_file_bert}\")\n",
    "\n",
    "# Save Tokenizer and Label Encoder\n",
    "with open('tokenizer_bert_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('label_encoder_bert_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "import os\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_val, y_pred_bert)\n",
    "labels = label_encoder.classes_\n",
    "classification_rep = classification_report(y_val, y_pred_bert, target_names=labels)\n",
    "\n",
    "# Save accuracy and classification report to a text file\n",
    "save_path = 'BERT_models'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "report_path = os.path.join(save_path, 'bert_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(f\"BERT Accuracy: {accuracy}\\n\\n\")\n",
    "    f.write(\"BERT Classification Report:\\n\")\n",
    "    f.write(classification_rep)\n",
    "\n",
    "print(f\"BERT report saved to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize texts\n",
    "max_len = 100  # Adjust according to your text length\n",
    "X = df['text'].astype(str).tolist()\n",
    "X = tokenizer(X, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X.input_ids.numpy(), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# BERT model architecture\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_output = bert_model(input_ids)[1]  # Taking pooled output as representation\n",
    "dropout = Dropout(0.1)(bert_output)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
    "model_bert = Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "model_bert.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train BERT model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_bert = model_bert.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate BERT model\n",
    "y_pred_probs_bert = model_bert.predict(X_val)\n",
    "y_pred_bert = np.argmax(y_pred_probs_bert, axis=1)\n",
    "\n",
    "# Save BERT model\n",
    "model_file_bert = 'BERT_models/bert_emotion_classifier.h5'\n",
    "model_bert.save(model_file_bert)\n",
    "print(f\"BERT Model saved as {model_file_bert}\")\n",
    "\n",
    "# Save Tokenizer and Label Encoder\n",
    "with open('tokenizer_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('label_encoder_bert.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "# Load dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv('data/combined_emotions.csv')\n",
    "df = df.sample(n=5000, random_state=42)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize texts\n",
    "max_len = 80  # Adjust according to your text length\n",
    "X = df['text'].astype(str).tolist()\n",
    "tokens = tokenizer(X, truncation=True, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "\n",
    "# Encode emotions\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['emotion'])\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "input_ids = tokens['input_ids'].numpy()\n",
    "attention_mask = tokens['attention_mask'].numpy()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n",
    "    input_ids, \n",
    "    attention_mask, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# BERT model architecture\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Taking pooled output as representation\n",
    "dropout = Dropout(0.1)(bert_output)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dropout)\n",
    "model_bert = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "model_bert.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train BERT model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_bert = model_bert.fit(\n",
    "    {'input_ids': X_train_ids, 'attention_mask': X_train_mask}, \n",
    "    y_train, \n",
    "    epochs=25, \n",
    "    batch_size=32, \n",
    "    validation_data=({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate BERT model\n",
    "y_pred_probs_bert = model_bert.predict({'input_ids': X_val_ids, 'attention_mask': X_val_mask})\n",
    "y_pred_bert = np.argmax(y_pred_probs_bert, axis=1)\n",
    "\n",
    "# Save BERT model\n",
    "model_file_bert = 'BERT_models/bert_emotion_classifier_v1.keras'\n",
    "model_bert.save(model_file_bert)\n",
    "print(f\"BERT Model saved as {model_file_bert}\")\n",
    "\n",
    "# Save Tokenizer and Label Encoder\n",
    "with open('tokenizer_bert_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('label_encoder_bert_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "import os\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(y_val, y_pred_bert)\n",
    "labels = label_encoder.classes_\n",
    "classification_rep = classification_report(y_val, y_pred_bert, target_names=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
